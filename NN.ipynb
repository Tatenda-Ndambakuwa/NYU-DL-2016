{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with torch7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch has a package for neural networks called 'nn'. It can be imported into torch using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = require 'nn';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of neural networks designed over the years. Most neural networks can be represented as directed acyclic graphs. However (just as a note), we'll see a class of neural networks called recurrent neural networks that can't be represented in that manner. The simplest type of neural networks are sequential feed-forward neural networks. Multilayer perceptrons and Convolutional Neural Networks do fall in that category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a sequential network, we use a super-module called 'nn.Sequential'. Other modules can be added to this super-module to create a sequential neural network. \n",
    "\n",
    "A variant to the sequential network is the parallel module which has multiple neural network pipelines running in parallel. Also, if you want even more power and to design recurrent neural networks you would use another package called the 'nngraph' package. But, we are getting ahead of ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sequential super-module\n",
    "Here's how you create a sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! However, our model is empty and can't do anything yet. \n",
    "## The Linear module\n",
    "Let's add a linear module to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> output]\n",
       "  (1): nn.Linear(3 -> 4)\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - empty\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.Linear(3 -> 4)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 4\n",
       "          weight : DoubleTensor - size: 4x3\n",
       "          gradWeight : DoubleTensor - size: 4x3\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 4\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "    }\n",
       "  output : DoubleTensor - empty\n",
       "}\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:add(nn.Linear(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that printed a lot of things. Let's ignore most of it for now and concentrate on the 3 and 4. So, it takes a vector of size 3 and embeds it into a vector space of 4 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear module is basically just:\n",
    "$$ Y = W^{\\top}X + b$$ where X, Y and b are vectors and W is a matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice in the output of the module, there are internal variables for W called weight and b called bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify if they actually work like that. Consider an initialization for X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1163\n",
       "-0.2036\n",
       "-0.2703\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward pass\n",
    "You can get the output from any torch module by doing a <module_name>:forward(). Let's do that here to get Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = model:forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4918\n",
       "-0.1836\n",
       " 0.5253\n",
       "-0.3506\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's replicate that as a math calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's access the weight of that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = model:get(1).weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0346  0.4622  0.0502\n",
       " 0.4883 -0.1885 -0.4888\n",
       "-0.4798  0.4865 -0.5142\n",
       "-0.3095  0.4759 -0.5465\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model:get(1) command accesses the first module of model which is the Linear module we just defined. Next we access the weight variable of that module by chaining a '.weight'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can get the bias of that linear module like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = model:get(1).bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3801\n",
       "-0.2973\n",
       " 0.4296\n",
       "-0.4374\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually do some math. The Linear module can be expressed in torch maths as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4918\n",
       "-0.1836\n",
       " 0.5253\n",
       "-0.3506\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- The * operator is overloaded with matrix multiplication\n",
    "W*X+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the output of the above model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4918\n",
       "-0.1836\n",
       " 0.5253\n",
       "-0.3506\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the forward pass gave us the output, a backward pass through a module will give us its backpropagated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:zeroGradParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2665\n",
       " 1.2361\n",
       "-1.4992\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2665\n",
       " 1.2361\n",
       "-1.4992\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:backward(X, torch.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> output]\n",
       "  (1): nn.Linear(3 -> 4)\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - size: 3\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.Linear(3 -> 4)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 4\n",
       "          weight : DoubleTensor - size: 4x3\n",
       "          gradWeight : DoubleTensor - size: 4x3\n",
       "          gradInput : DoubleTensor - size: 3\n",
       "          bias : DoubleTensor - size: 4\n",
       "          output : DoubleTensor - size: 4\n",
       "        }\n",
       "    }\n",
       "  output : DoubleTensor - size: 4\n",
       "}\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1163 -0.2036 -0.2703\n",
       "-0.1163 -0.2036 -0.2703\n",
       "-0.1163 -0.2036 -0.2703\n",
       "-0.1163 -0.2036 -0.2703\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2665\n",
       " 1.2361\n",
       "-1.4992\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2664\t\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0346+0.4883+-0.4798+-0.3095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
