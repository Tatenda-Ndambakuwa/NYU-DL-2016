{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with torch7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch has a package for neural networks called 'nn'. It can be imported into torch using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = require 'nn';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of neural networks designed over the years. Most neural networks can be represented as directed acyclic graphs. However (just as a note), we'll see a class of neural networks called recurrent neural networks that can't be represented in that manner. The simplest type of neural networks are sequential feed-forward neural networks. Multilayer perceptrons and Convolutional Neural Networks do fall in that category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a sequential network, we use a super-module called 'nn.Sequential'. Other modules can be added to this super-module to create a sequential neural network. \n",
    "\n",
    "A variant to the sequential network is the parallel module which has multiple neural network pipelines running in parallel. Also, if you want even more power and to design recurrent neural networks you would use another package called the 'nngraph' package. But, we are getting ahead of ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sequential super-module\n",
    "Here's how you create a sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! However, our model is empty and can't do anything yet. \n",
    "## The Linear module\n",
    "Let's add a linear module to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> output]\n",
       "  (1): nn.Linear(3 -> 4)\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - empty\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.Linear(3 -> 4)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 4\n",
       "          weight : DoubleTensor - size: 4x3\n",
       "          gradWeight : DoubleTensor - size: 4x3\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 4\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "    }\n",
       "  output : DoubleTensor - empty\n",
       "}\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:add(nn.Linear(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that printed a lot of things. Let's ignore most of it for now and concentrate on the 3 and 4. So, it takes a vector of size 3 and embeds it into a vector space of 4 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear module is basically just:\n",
    "$$ Y = W^{\\top}X + B$$ where X, Y and B are vectors and W is a matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice in the output of the module, there are internal variables for W called weight and B called bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify if they actually work like that. Consider an initialization for X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8625\n",
       " 1.2032\n",
       " 0.7942\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward pass\n",
    "You can get the output from any torch module by doing a <module_name>:forward(). Let's do that here to get Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = model:forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2760\n",
       " 0.0477\n",
       " 0.1237\n",
       " 0.8479\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's replicate that as a math calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's access the weight of that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = model:get(1).weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0767 -0.4704  0.3049\n",
       "-0.0658 -0.2164 -0.1403\n",
       " 0.2269  0.2260  0.2707\n",
       "-0.5127  0.0128 -0.1652\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model:get(1) command accesses the first module of model which is the Linear module we just defined. Next we access the weight variable of that module by chaining a '.weight'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can get the bias of that linear module like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = model:get(1).bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0182\n",
       " 0.3628\n",
       "-0.1674\n",
       " 0.5215\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually do some math. The Linear module can be expressed in torch maths as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2760\n",
       " 0.0477\n",
       " 0.1237\n",
       " 0.8479\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- The * operator is overloaded with matrix multiplication\n",
    "W*X+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the output of the above model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2760\n",
       " 0.0477\n",
       " 0.1237\n",
       " 0.8479\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the modules store their previous outputs in the output variable. This is useful in calculating the gradient later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the forward pass gave us the output, a backward pass through a module will give us its backpropagated gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything, we should clear our gradient parameters. What gradient parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the internals of model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> output]\n",
       "  (1): nn.Linear(3 -> 4)\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - empty\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.Linear(3 -> 4)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 4\n",
       "          weight : DoubleTensor - size: 4x3\n",
       "          gradWeight : DoubleTensor - size: 4x3\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 4\n",
       "          output : DoubleTensor - size: 4\n",
       "        }\n",
       "    }\n",
       "  output : DoubleTensor - size: 4\n",
       "}\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice the grad variables? Here's a mapping between these variables and mathematical expressions:  \n",
    "* $$gradBias = \\frac{\\partial E}{\\partial B}$$\n",
    "* $$gradWeight = \\frac{\\partial E}{\\partial W}$$\n",
    "* $$gradInput = \\frac{\\partial E}{\\partial X}$$\n",
    "if E is the final Energy/Error of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch's :backward() function allows you to calculate the above values easily. However, we need to perform one operation before doing that, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:zeroGradParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essential because sometimes garbage values in the grad parameters spoil your gradients. Now, let's check out the values of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.DoubleTensor with no dimension]\n",
       "\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradOutput = torch.Tensor{1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5783\n",
       "-0.1742\n",
       " 0.1756\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- format --> model:backward(input_passed, error_that_came_from_previous_modules)\n",
    "\n",
    "model:backward(X, gradOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second parameter of backward (gradOutput) contains the error passed down from the layers above this layer. However, since we don't have any modules above the Linear module, we pass a vector of 2s (just for a demonstration). The mapping between this and a mathematical expression is:\n",
    "$$gradOutput = \\frac{\\partial E}{\\partial Y}$$\n",
    "if E is the final Energy/Error of your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, let's confirm that the values are being calculated as per their respective mathematical expressions.\n",
    "\n",
    "First let's write down the expression for Y.\n",
    "$$Y = \n",
    "\\begin{bmatrix}\n",
    "w_{11} x_1 + w_{12} x_2 + w_{13} x_3 + b_1\\\\\n",
    "w_{21} x_1 + w_{22} x_2 + w_{23} x_3 + b_2\\\\\n",
    "w_{31} x_1 + w_{32} x_2 + w_{33} x_3 + b_3\\\\\n",
    "w_{41} x_1 + w_{42} x_2 + w_{43} x_3 + b_4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to analytical expression for $\\frac{\\partial E}{\\partial B}$, \n",
    "$$ \\frac{\\partial E}{\\partial B} = \\frac{\\partial E}{\\partial Y} \\frac{\\partial Y}{\\partial B}$$\n",
    "Now, $\\frac{\\partial Y}{\\partial B}$ is a jacobian of vector Y w.r.t. vector B. This is actually equal to the identity matrix I. Therefore, \n",
    "$$ \\frac{\\partial E}{\\partial B} = \\frac{\\partial E}{\\partial Y} I = \\frac{\\partial E}{\\partial Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       "[torch.DoubleTensor of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradBias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to analytical derivation,\n",
    "$$\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial Y} \\frac{\\partial Y}{\\partial W} = \\frac{\\partial E}{\\partial Y} X = \\begin{bmatrix} x_1 \\frac{\\partial E}{\\partial y_1} & x_2 \\frac{\\partial E}{\\partial y_1} & x_3 \\frac{\\partial E}{\\partial y_1} \\\\\n",
    "x_1 \\frac{\\partial E}{\\partial y_2} & x_2 \\frac{\\partial E}{\\partial y_2} & x_3 \\frac{\\partial E}{\\partial y_2} \\\\\n",
    "x_1 \\frac{\\partial E}{\\partial y_3} & x_2 \\frac{\\partial E}{\\partial y_3} & x_3 \\frac{\\partial E}{\\partial y_3} \\\\\n",
    "x_1 \\frac{\\partial E}{\\partial y_4} & x_2 \\frac{\\partial E}{\\partial y_4} & x_3 \\frac{\\partial E}{\\partial y_4}\n",
    "\\end{bmatrix}$$\n",
    "We perform the top three operations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8625  1.2032  0.7942\n",
       "-1.7249  2.4064  1.5884\n",
       "-2.5874  3.6097  2.3826\n",
       "-3.4499  4.8129  3.1768\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradOutput:reshape(4,1)*X:reshape(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8625  1.2032  0.7942\n",
       "-1.7249  2.4064  1.5884\n",
       "-2.5874  3.6097  2.3826\n",
       "-3.4499  4.8129  3.1768\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical expression for $\\frac{\\partial E}{\\partial X}$ is:\n",
    "$$\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y}\\frac{\\partial Y}{\\partial X} = \\begin{bmatrix}\n",
    "    \\frac{\\partial E}{\\partial y_1}  & \\frac{\\partial E}{\\partial y_2}  & \\frac{\\partial E}{\\partial y_3} & \\frac{\\partial E}{\\partial y_4} \n",
    "\\end{bmatrix} \\times \n",
    "\\begin{bmatrix}\n",
    "    w_{11} & w_{12} & w_{13} \\\\\n",
    "    w_{21} & w_{22} & w_{23} \\\\\n",
    "    w_{31} & w_{32} & w_{33} \\\\\n",
    "    w_{41} & w_{42} & w_{43} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To confirm the above expression, here's the weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3449 -0.5608 -0.0103\n",
       "-0.1104  0.0342 -0.2782\n",
       " 0.0876  0.3920  0.2362\n",
       " 0.0042  0.3910  0.5008\n",
       "[torch.DoubleTensor of size 4x3]\n",
       "\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform the calculation mentioned above, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5783\n",
       "-0.1742\n",
       " 0.1756\n",
       "[torch.DoubleTensor of size 3x1]\n",
       "\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W:transpose(1,2)*gradOutput:reshape(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5783\n",
       "-0.1742\n",
       " 0.1756\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model:get(1).gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
